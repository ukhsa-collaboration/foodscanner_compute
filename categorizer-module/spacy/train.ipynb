{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from pathlib import Path\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import spacy\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from spacy.training import docs_to_json\r\n",
        "\r\n",
        "%load_ext nb_black\r\n",
        "\r\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "outputs": [],
      "metadata": {
        "id": "5AHZulflnLPY",
        "gather": {
          "logged": 1627681450806
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation\n",
        "\n",
        "- Use `dtype` for performance\n",
        "- `ingredients` is a string of ingredients delimeted with `|`, replace with `.`. Fill empty cells with `None` as the lack of ingredients is significant\n",
        "- `cooking_type` is a string of cooking types categories delimeted with `|`, replace with `.`. However, before replacing, fill empty cells with `None` as the lack of a cooking type is significant\n",
        "- Some products have duplicated `description`. To remove them, we set `pvid` as the index and sort it in an ascending order, then drop rows with duplicated `description` but keeping the one with the last `pvid` (i.e. the most recent product)\n",
        "- Drop rows with any empty cell. `ingredients` and `cooking_type` empty cells are now `None` so will not be dropped\n",
        "- Concatenate all columns using '. ' into new column `text`"
      ],
      "metadata": {
        "id": "N7YbliXbnLPf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def process_file(file: str) -> pd.DataFrame:\r\n",
        "    \"\"\"Process data for training\r\n",
        "\r\n",
        "    Args:\r\n",
        "        file (str): file to process\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        pd.DataFrame: processed dataFrame\r\n",
        "    \"\"\"\r\n",
        "    return (\r\n",
        "        pd.read_excel(\r\n",
        "            Path(\r\n",
        "                \"data\",\r\n",
        "                file,\r\n",
        "            ),\r\n",
        "            usecols=[\r\n",
        "                \"lProductVersionID\",\r\n",
        "                \"sDescription\",\r\n",
        "                \"sCategoryLevel1\",\r\n",
        "                \"sCategoryLevel2\",\r\n",
        "                \"regulated_product_name\",\r\n",
        "                \"ingredients\",\r\n",
        "                \"storage_env\",\r\n",
        "                \"pack_type\",\r\n",
        "                \"cooking_type\",\r\n",
        "                \"PHE_category\",\r\n",
        "            ],\r\n",
        "            dtype={\r\n",
        "                \"lProductVersionID\": \"uint64\",\r\n",
        "                \"sDescription\": str,\r\n",
        "                \"sCategoryLevel1\": \"category\",\r\n",
        "                \"sCategoryLevel2\": \"category\",\r\n",
        "                \"regulated_product_name\": str,\r\n",
        "                \"ingredients\": str,\r\n",
        "                \"storage_env\": \"category\",\r\n",
        "                \"pack_type\": \"category\",\r\n",
        "                \"cooking_type\": str,\r\n",
        "                \"PHE_category\": \"category\",\r\n",
        "            },\r\n",
        "        )\r\n",
        "        .rename(\r\n",
        "            columns={\r\n",
        "                \"lProductVersionID\": \"pvid\",\r\n",
        "                \"sDescription\": \"description\",\r\n",
        "                \"sCategoryLevel1\": \"category_level_1\",\r\n",
        "                \"sCategoryLevel2\": \"category_level_2\",\r\n",
        "                \"PHE_category\": \"label\",\r\n",
        "            }\r\n",
        "        )\r\n",
        "        .assign(\r\n",
        "            ingredients=lambda df: df[\"ingredients\"]\r\n",
        "            .str.replace(\"|\", \".\", regex=False)\r\n",
        "            .fillna(\"None\"),\r\n",
        "            cooking_type=lambda df: df[\"cooking_type\"]\r\n",
        "            .fillna(\"None\")\r\n",
        "            .str.replace(\"|\", \".\", regex=False),\r\n",
        "            label=lambda df: df[\"label\"].str.lower(),\r\n",
        "        )\r\n",
        "        .set_index(\r\n",
        "            \"pvid\",\r\n",
        "        )\r\n",
        "        .sort_index(\r\n",
        "            ascending=True,\r\n",
        "        )\r\n",
        "        .drop_duplicates(\r\n",
        "            subset=\"description\",\r\n",
        "            keep=\"last\",\r\n",
        "        )\r\n",
        "        .dropna(\r\n",
        "            how=\"any\",\r\n",
        "        )\r\n",
        "        .assign(\r\n",
        "            text=lambda df: df.apply(\r\n",
        "                \". \".join,\r\n",
        "                axis=1,\r\n",
        "            )\r\n",
        "        )\r\n",
        "    )\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "1rHqV3jAnLPg",
        "gather": {
          "logged": 1627681451265
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Data\r\n",
        "- Get unique labels\r\n",
        "- Concat labels for examples with multi-labels"
      ],
      "metadata": {
        "id": "X9SRbjb1nLPn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = process_file(\"210714_updated_clean_sheet.xlsx\")\r\n",
        "df.info()\r\n",
        "\r\n",
        "# check labels frequencies\r\n",
        "df[\"label\"].value_counts().to_dict()\r\n",
        "\r\n",
        "# move labels with low frequencies to other\r\n",
        "df[\"label\"] = df[\"label\"].replace(\r\n",
        "    to_replace=[\"other_sauces\", \"cream_alternative\"],\r\n",
        "    value=\"other\",\r\n",
        ")\r\n",
        "\r\n",
        "labels = df[\"label\"].unique()"
      ],
      "outputs": [],
      "metadata": {
        "id": "06slzyJhnLPo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = process_file(\"210714_updated_clean_sheet-simple.xlsx\")\r\n",
        "df.info()\r\n",
        "\r\n",
        "# check labels frequencies\r\n",
        "df[\"label\"].value_counts().to_dict()\r\n",
        "\r\n",
        "# move labels with low frequencies to other\r\n",
        "df[\"label\"] = df[\"label\"].replace(\r\n",
        "    to_replace=[\"oils\", \"prepared_soups\"],\r\n",
        "    value=\"other\",\r\n",
        ")\r\n",
        "\r\n",
        "labels = df[\"label\"].unique()"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1627681474815
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### - Convert text and labels into a SpaCy compatible format"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def convert_to_spacy(s, labels):\r\n",
        "    \"\"\"\r\n",
        "    Convert text and labels into a spaCy compitable format\r\n",
        "    \"\"\"\r\n",
        "    # docs_to_json expects a dict of cats\r\n",
        "    cats = {label: 1.0 if label in s[\"multilabel\"] else 0.0 for label in labels}\r\n",
        "\r\n",
        "    doc = nlp(s[\"text\"])\r\n",
        "    doc.cats = cats\r\n",
        "\r\n",
        "    return docs_to_json([doc])"
      ],
      "outputs": [],
      "metadata": {
        "id": "0ihpRQlRnLQD",
        "gather": {
          "logged": 1627681475137
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# concat labels for examples with multi-labels\r\n",
        "df = (\r\n",
        "    df.groupby(\"text\")[\"label\"]\r\n",
        "    .apply(set)\r\n",
        "    .reset_index()\r\n",
        "    .rename(columns={\"label\": \"multilabel\"})\r\n",
        ")\r\n",
        "\r\n",
        "# for binary classification, resolve/drop ambiguous rows with multi-lables\r\n",
        "len(df[df[\"multilabel\"].apply(len) > 1])\r\n",
        "df = df[df[\"multilabel\"].apply(len) == 1]\r\n",
        "\r\n",
        "df['spacy'] = df.apply(\r\n",
        "    lambda s: convert_to_spacy(s, labels),\r\n",
        "    axis=1,\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "MQHQAc4LnLQK",
        "gather": {
          "logged": 1627682363843
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### - Split data 70/30 for train/val and save results into json files"
      ],
      "metadata": {
        "id": "dpkyIqBNnLQR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def split_save_json(df, test_size):\r\n",
        "    \"\"\"\r\n",
        "    Split data 30/70 and stratify by label\r\n",
        "    Save into json\r\n",
        "    \"\"\"\r\n",
        "    train, val = train_test_split(\r\n",
        "        df['spacy'],\r\n",
        "        test_size=test_size,\r\n",
        "        random_state=42,\r\n",
        "        shuffle=True,\r\n",
        "    )\r\n",
        "\r\n",
        "    train.to_json(\r\n",
        "        Path(\r\n",
        "            \"spacy\",\r\n",
        "            \"assets\",\r\n",
        "            'train.json',\r\n",
        "        ),\r\n",
        "        orient='records',\r\n",
        "    )\r\n",
        "\r\n",
        "    val.to_json(\r\n",
        "        Path(\r\n",
        "            \"spacy\",\r\n",
        "            \"assets\",\r\n",
        "            'dev.json',\r\n",
        "        ),\r\n",
        "        orient='records',\r\n",
        "    )"
      ],
      "outputs": [],
      "metadata": {
        "id": "3--ClLOZnLQR",
        "gather": {
          "logged": 1627682364205
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "split_save_json(df, test_size=0.3)"
      ],
      "outputs": [],
      "metadata": {
        "id": "DzmnlsBOnLQW",
        "gather": {
          "logged": 1627682381837
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Validation\r\n",
        "\r\n",
        "- AUC ROC score:\r\n",
        "    - Training: $100\\%$\r\n",
        "    - Testing: $99\\%$\r\n",
        "    "
      ],
      "metadata": {
        "id": "8izADo6SnLQZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!spacy project run all"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "model.ipynb",
      "provenance": []
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}