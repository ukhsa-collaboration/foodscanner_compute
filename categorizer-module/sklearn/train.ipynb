{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "from pathlib import Path\r\n",
        "\r\n",
        "import joblib\r\n",
        "import pandas as pd\r\n",
        "import xgboost as xgb\r\n",
        "\r\n",
        "from sklearn.compose import ColumnTransformer\r\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\r\n",
        "from sklearn.feature_extraction import FeatureHasher\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.metrics import balanced_accuracy_score, multilabel_confusion_matrix\r\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\r\n",
        "from sklearn import set_config\r\n",
        "set_config(display='diagram')\r\n",
        "%load_ext nb_black "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The nb_black extension is already loaded. To reload it, use:\n",
            "  %reload_ext nb_black\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 14;\n                var nbb_unformatted_code = \"from pathlib import Path\\r\\n\\r\\nimport joblib\\r\\nimport pandas as pd\\r\\nimport xgboost as xgb\\r\\n\\r\\nfrom sklearn.compose import ColumnTransformer\\r\\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\\r\\nfrom sklearn.feature_extraction import FeatureHasher\\r\\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\r\\nfrom sklearn.linear_model import LogisticRegression\\r\\nfrom sklearn.metrics import balanced_accuracy_score, multilabel_confusion_matrix\\r\\nfrom sklearn.model_selection import GridSearchCV, train_test_split\\r\\nfrom sklearn.naive_bayes import MultinomialNB\\r\\nfrom sklearn.pipeline import Pipeline\\r\\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\\r\\nfrom sklearn.svm import LinearSVC\\r\\nfrom sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\\r\\nfrom sklearn import set_config\\r\\nset_config(display='diagram')\\r\\n%load_ext nb_black \";\n                var nbb_formatted_code = \"from pathlib import Path\\n\\nimport joblib\\nimport pandas as pd\\nimport xgboost as xgb\\n\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\\nfrom sklearn.feature_extraction import FeatureHasher\\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import balanced_accuracy_score, multilabel_confusion_matrix\\nfrom sklearn.model_selection import GridSearchCV, train_test_split\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\\nfrom sklearn.svm import LinearSVC\\nfrom sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\\nfrom sklearn import set_config\\n\\nset_config(display=\\\"diagram\\\")\\n%load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "gather": {
          "logged": 1627579722153
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "df = pd.read_csv(\r\n",
        "    Path(\r\n",
        "        \"sklearn\",\r\n",
        "        \"assets\",\r\n",
        "        # \"dataset.csv\",\r\n",
        "        \"dataset-simple.csv\",\r\n",
        "    ),\r\n",
        "    index_col='pvid',\r\n",
        ")\r\n",
        "\r\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        category_level_1   category_level_2 storage_env       pack_type  \\\n",
              "pvid                                                                      \n",
              "6345063          Grocery         Other Tins     Ambient             Can   \n",
              "6345067      Off Licence        Other Wines     Ambient          Bottle   \n",
              "6345069           Drinks  Carbonated Drinks     Ambient  Plastic Bottle   \n",
              "6345073      Off Licence           Red Wine     Ambient          Bottle   \n",
              "6345074           Drinks  Carbonated Drinks     Ambient             Can   \n",
              "\n",
              "        cooking_type   label  \\\n",
              "pvid                           \n",
              "6345063     ['None']   other   \n",
              "6345067     ['None']    none   \n",
              "6345069     ['None']  drinks   \n",
              "6345073     ['None']    none   \n",
              "6345074     ['None']  drinks   \n",
              "\n",
              "                                                      text  \n",
              "pvid                                                        \n",
              "6345063  Coco Fresh Coconut Milk 400ml. Coconut Milk. W...  \n",
              "6345067  Cockburn's Fine Tawny Port 75cl. Fine Tawny Po...  \n",
              "6345069  Coca-Cola Zero Cherry 2 Litre. Sparkling Low C...  \n",
              "6345073  Co Op Fairtrade Carmenère 75cl. Carmenère - Re...  \n",
              "6345074  Coca-Cola Zero Sugar Raspberry 250ml. Sparklin...  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category_level_1</th>\n",
              "      <th>category_level_2</th>\n",
              "      <th>storage_env</th>\n",
              "      <th>pack_type</th>\n",
              "      <th>cooking_type</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pvid</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6345063</th>\n",
              "      <td>Grocery</td>\n",
              "      <td>Other Tins</td>\n",
              "      <td>Ambient</td>\n",
              "      <td>Can</td>\n",
              "      <td>['None']</td>\n",
              "      <td>other</td>\n",
              "      <td>Coco Fresh Coconut Milk 400ml. Coconut Milk. W...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6345067</th>\n",
              "      <td>Off Licence</td>\n",
              "      <td>Other Wines</td>\n",
              "      <td>Ambient</td>\n",
              "      <td>Bottle</td>\n",
              "      <td>['None']</td>\n",
              "      <td>none</td>\n",
              "      <td>Cockburn's Fine Tawny Port 75cl. Fine Tawny Po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6345069</th>\n",
              "      <td>Drinks</td>\n",
              "      <td>Carbonated Drinks</td>\n",
              "      <td>Ambient</td>\n",
              "      <td>Plastic Bottle</td>\n",
              "      <td>['None']</td>\n",
              "      <td>drinks</td>\n",
              "      <td>Coca-Cola Zero Cherry 2 Litre. Sparkling Low C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6345073</th>\n",
              "      <td>Off Licence</td>\n",
              "      <td>Red Wine</td>\n",
              "      <td>Ambient</td>\n",
              "      <td>Bottle</td>\n",
              "      <td>['None']</td>\n",
              "      <td>none</td>\n",
              "      <td>Co Op Fairtrade Carmenère 75cl. Carmenère - Re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6345074</th>\n",
              "      <td>Drinks</td>\n",
              "      <td>Carbonated Drinks</td>\n",
              "      <td>Ambient</td>\n",
              "      <td>Can</td>\n",
              "      <td>['None']</td>\n",
              "      <td>drinks</td>\n",
              "      <td>Coca-Cola Zero Sugar Raspberry 250ml. Sparklin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 23;\n                var nbb_unformatted_code = \"df = pd.read_csv(\\r\\n    Path(\\r\\n        \\\"sklearn\\\",\\r\\n        \\\"assets\\\",\\r\\n        # \\\"dataset.csv\\\",\\r\\n        \\\"dataset-simple.csv\\\",\\r\\n    ),\\r\\n    index_col='pvid',\\r\\n)\\r\\n\\r\\ndf.head()\";\n                var nbb_formatted_code = \"df = pd.read_csv(\\n    Path(\\n        \\\"sklearn\\\",\\n        \\\"assets\\\",\\n        # \\\"dataset.csv\\\",\\n        \\\"dataset-simple.csv\\\",\\n    ),\\n    index_col=\\\"pvid\\\",\\n)\\n\\ndf.head()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1627579746121
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split data\n",
        "\n",
        "- Use `label` as target labels\n",
        "    - Number of classes is $63$\n",
        "    - **Classes are very imbalanced**\n",
        "        - Estimate class weights by using $n_{samples} / (n_{classes} * np.bincount(y))$ (not used but in case needed)\n",
        "        - Calculate sample weights (not used but in case needed)\n",
        "    - Stratify labels when splitting so their distribution in train/test data is similar\n",
        "- Encode target labels with values between $0-62$\n",
        "- Split $70/30$ for training/testing\n",
        "- Create empty dict to store all classifiers. Once populated, an item will look like:\n",
        "`'classifier name' : {\n",
        "    'pipeline': ...,\n",
        "    'params': ...,\n",
        "    'best_score': ...,\n",
        "    'best_params': ...,\n",
        "    'best_estimator': ...,\n",
        "    'best_estimator_params': ...,\n",
        "    'testing_accuracy': ...,\n",
        "    'testing_conf_matrix': ...,\n",
        "}`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "y = df[\"label\"]\r\n",
        "num_class = len(y.unique())\r\n",
        "\r\n",
        "class_weights = compute_class_weight(\r\n",
        "    class_weight=\"balanced\",\r\n",
        "    classes=y.unique(),\r\n",
        "    y=y,\r\n",
        ")\r\n",
        "\r\n",
        "sample_weights = compute_sample_weight(\r\n",
        "    class_weight=\"balanced\",\r\n",
        "    y=y,\r\n",
        ")\r\n",
        "\r\n",
        "X = df.drop(\"label\", axis=1)\r\n",
        "\r\n",
        "le = LabelEncoder()\r\n",
        "y = le.fit_transform(y)\r\n",
        "\r\n",
        "if not Path(\"models\").is_dir():\r\n",
        "    Path(\"models\").mkdir()\r\n",
        "\r\n",
        "joblib.dump(\r\n",
        "    le,\r\n",
        "    Path(\r\n",
        "        \"sklearn\",\r\n",
        "        \"models\",\r\n",
        "        \"LabelEncoder.pkl\",\r\n",
        "    ),\r\n",
        ")\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(\r\n",
        "    X,\r\n",
        "    y,\r\n",
        "    test_size=0.3,\r\n",
        "    random_state=42,\r\n",
        "    shuffle=True,\r\n",
        "    stratify=y,\r\n",
        ")\r\n",
        "\r\n",
        "classifiers = dict()"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1627579793646
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers\n",
        "\n",
        "### Create 4 transformation `Pipeline`:\n",
        "\n",
        "1. A pipeline for text features: `text`. This will have 1 step:\n",
        "    - `TfidfVectorizer` which is equivalent to `CountVectorizer` followed by `TfidfTransformer`\n",
        "\n",
        "2. A pipeline for categorical features with 1 label each: `category_level_1`, `category_level_2`, `storage_env`. This will have 1 step:\n",
        "    - `OneHotEncoder`\n",
        "\n",
        "3. A pipeline for categorical features with multiple labels each: `cooking_type`. This will have 1 step:\n",
        "    - `CountVectorizer`\n",
        "\n",
        "4. A pipeline for categorical features to be hashed: `pack_type`. This will have 1 step:\n",
        "    - `FeatureHasher`. We used the hashing trick on `pack_type` as it has 45 categories and using `OneHotEncoder` would result in 45 sparse features\n",
        "    - Use a power of 2 for `n_features`\n",
        "    - Collisions are likely to cancel out rather than accumulate error when `alternate_sign=True`. However, MultinomialNB estimators expect non-negative inputs so will disable `alternate_sign`\n",
        "\n",
        "- `remainder=drop` will be used to drop any extra features that might be added to the dataframe later as a safety guard. When adding new features, either pass them through a pipeline, or change to `remainder=passthrough`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "text_transformer = Pipeline(\r\n",
        "    steps=[\r\n",
        "        (\r\n",
        "            \"tfidf\",\r\n",
        "            TfidfVectorizer(\r\n",
        "                lowercase=True, ngram_range=(1, 2), norm=\"l2\", use_idf=True\r\n",
        "            ),\r\n",
        "        )\r\n",
        "    ]\r\n",
        ")\r\n",
        "\r\n",
        "cat_transformer = Pipeline(\r\n",
        "    steps=[\r\n",
        "        (\r\n",
        "            \"onehot\",\r\n",
        "            OneHotEncoder(categories=\"auto\", sparse=False, handle_unknown=\"ignore\"),\r\n",
        "        )\r\n",
        "    ]\r\n",
        ")\r\n",
        "\r\n",
        "multi_cat_transformer = Pipeline(steps=[(\"binarizer\", CountVectorizer(analyzer=set))])\r\n",
        "\r\n",
        "hash_transformer = Pipeline(\r\n",
        "    steps=[\r\n",
        "        (\r\n",
        "            \"hasher\",\r\n",
        "            FeatureHasher(n_features=10, input_type=\"string\", alternate_sign=False),\r\n",
        "        )\r\n",
        "    ]\r\n",
        ")\r\n",
        "\r\n",
        "preprocessor = ColumnTransformer(\r\n",
        "    transformers=[\r\n",
        "        (\"text\", text_transformer, \"text\"),\r\n",
        "        (\r\n",
        "            \"cat\",\r\n",
        "            cat_transformer,\r\n",
        "            [\"category_level_1\", \"category_level_2\", \"storage_env\"],\r\n",
        "        ),\r\n",
        "        (\"multi_cat\", multi_cat_transformer, \"cooking_type\"),\r\n",
        "        (\"hash\", hash_transformer, \"pack_type\"),\r\n",
        "    ],\r\n",
        "    remainder=\"drop\",\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1627579815635
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check for Collisions\n",
        "\n",
        "- Check for possible collisions in the hashing features for `pack_type`\n",
        "- Increase `n_features` by 2 until no duplicated hashing features exist"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "hashed_features = (\r\n",
        "    hash_transformer.named_steps[\"hasher\"].fit_transform(df[\"pack_type\"]).toarray()\r\n",
        ")\r\n",
        "\r\n",
        "df_hashed_features = (\r\n",
        "    df[[\"pack_type\"]].reset_index(drop=True).join(pd.DataFrame(hashed_features))\r\n",
        ")\r\n",
        "\r\n",
        "df_hashed_features.groupby([\"pack_type\"]).first().duplicated(keep=False).sum()"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1627579818193
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifiers"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multinomial Naïve Bayes\n",
        "\n",
        "- Normally requires bow\n",
        "- tf-idf vectors are also known to work well in practice\n",
        "- `alpha`: additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "classifier = MultinomialNB(alpha=1.0)\r\n",
        "\r\n",
        "pipeline = Pipeline(\r\n",
        "    steps=[\r\n",
        "        ('preprocessor', preprocessor),\r\n",
        "        ('classifier', classifier),\r\n",
        "    ]\r\n",
        ")\r\n",
        "\r\n",
        "params = {\r\n",
        "    'classifier__alpha': [0.001, 0.01, 0.1, 0.3, 0.5, 1],\r\n",
        "}\r\n",
        "\r\n",
        "classifiers['MultinomialNB'] = {\r\n",
        "    'pipeline': pipeline,\r\n",
        "    'params': params,\r\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1627579820564
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pipeline"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1627579822713
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "\n",
        "- `multinomial` logistic regression yields more accurate results and is faster to train on larger scale dataset than `ovr` logistic regression\n",
        "- `saga` solver works with `multinomial` and is faster for large datasets\n",
        "- `penalty=l1` trims the weights of not informative features to zero which is good if the goal is to extract the strongly discriminative vocabulary of each class. However, to get the best predictive accuracy, it is better to use `penalty=l2` instead"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "classifier = LogisticRegression(\r\n",
        "    solver='saga',\r\n",
        "    class_weight='balanced',\r\n",
        "    multi_class='multinomial',\r\n",
        "    penalty='l2',\r\n",
        "    random_state=42,\r\n",
        "    max_iter=100,\r\n",
        ")\r\n",
        "\r\n",
        "pipeline = Pipeline(\r\n",
        "    steps=[\r\n",
        "        ('preprocessor', preprocessor),\r\n",
        "        ('classifier', classifier),\r\n",
        "    ]\r\n",
        ")\r\n",
        "\r\n",
        "params = {\r\n",
        "    'classifier__C': [1, 10, 100],\r\n",
        "}\r\n",
        "\r\n",
        "classifiers['LogisticRegression'] = {\r\n",
        "    'pipeline': pipeline,\r\n",
        "    'params': params,\r\n",
        "}\r\n"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1627579825919
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pipeline"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1627579827244
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Support Vector Machine\n",
        "\n",
        "- LinearSVC is Similar to SVC with parameter `kernel=linear` but scales better\n",
        "to large number of samples\n",
        "- Select `dual=True` when `n_samples` < `n_features`. The TfidfVectorizer creates a large sparse matrix as we are using uni-grams and bi-grams (more than 200K sparse features)\n",
        "- Select `multi_class=ovr` to train one-vs-rest classifiers\n",
        "- `class_weight=balanced`: automatically adjust weights inversely proportional to class frequencies\n",
        "- `C`: regularization parameter. Higher values result in less regularization (narrower margin with fewer violations)\n",
        "- Advantages\n",
        "    - Good for linear and non-linear classification\n",
        "    - Well suited for classification of complex but small to medium datasets\n",
        "- Disadvantages\n",
        "    - Speed and datasets size\n",
        "    - Needs scaling to be centred around zero\n",
        "    - Sensitive to outliers"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "classifier = LinearSVC(\r\n",
        "    penalty='l2',\r\n",
        "    loss='squared_hinge',\r\n",
        "    # dual=True,\r\n",
        "    multi_class='ovr',\r\n",
        "    class_weight='balanced',\r\n",
        "    random_state=42,\r\n",
        "    max_iter=1000,\r\n",
        ")\r\n",
        "\r\n",
        "pipeline = Pipeline(\r\n",
        "    steps=[\r\n",
        "        ('preprocessor', preprocessor),\r\n",
        "        ('classifier', classifier),\r\n",
        "    ]\r\n",
        ")\r\n",
        "\r\n",
        "params = {\r\n",
        "    'classifier__C': [1, 10, 100],\r\n",
        "    'classifier__dual': [True, False],\r\n",
        "}\r\n",
        "\r\n",
        "classifiers['LinearSVC'] = {\r\n",
        "    'pipeline': pipeline,\r\n",
        "    'params': params,\r\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1627579830650
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pipeline"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1627579832989
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Ensemble\n",
        "\n",
        "- Each tree is trained on a random subset of the training set with replacement (i.e. bootstrap aggregating or bagging)\n",
        "- Each tree is trained on a random subset of features, the number of features to use is defined by `max_features`\n",
        "- Advantages\n",
        "    - No need for scaling\n",
        "    - No need for dimensionality reduction (unless rotation is needed)\n",
        "- Disadvantages\n",
        "    - Sensitive to small variations in the training data\n",
        "    - Over-fitting\n",
        "    - Difficult to interpret"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "classifier = RandomForestClassifier(\r\n",
        "    bootstrap=True,\r\n",
        "    oob_score=True,\r\n",
        "    max_features='sqrt',\r\n",
        "    class_weight='balanced',\r\n",
        "    random_state=42,\r\n",
        ")\r\n",
        "\r\n",
        "pipeline = Pipeline(\r\n",
        "    steps=[\r\n",
        "        ('preprocessor', preprocessor),\r\n",
        "        ('classifier', classifier),\r\n",
        "    ]\r\n",
        ")\r\n",
        "\r\n",
        "params = {\r\n",
        "    'classifier__criterion': ['gini', 'entropy'],\r\n",
        "    'classifier__max_depth': [4, 5, 6],\r\n",
        "    'classifier__n_estimators': [100, 200, 400],\r\n",
        "}\r\n",
        "\r\n",
        "classifiers['RandomForestClassifier'] = {\r\n",
        "    'pipeline': pipeline,\r\n",
        "    'params': params,\r\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1627579834798
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pipeline"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1627579840255
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stochastic Extreme Gradient Boosting (XGBoost) Ensemble\n",
        "\n",
        "- Boosting methods train predictors sequentially, each trying to correct its predecessor. Gradient Boosting tries to fit the new predictor to the residual error made by the previous one. Extreme Boosting is more regularized to control over-fitting, which gives it better performance\n",
        "- XGBoost uses trees as the base booster by default (booster=`gbtree`), which has a sklearn API (other booster don't)\n",
        "- `learning_rate`: scales the contribution of each tree (ASA shrinkage). Lower `learning_rate` requires more trees to fit the training data\n",
        "- `subsample`: ratio of training data to be randomly sampled to train each tree (i.e. Stochastic). Typically, set `subsample >= 0.5` for good results when `sampling_method=uniform` which is the default\n",
        "- `colsample_bytree`: ratio of columns to be randomly sampled prior to train each tree"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "classifier = xgb.XGBClassifier(\r\n",
        "    booster='gbtree',\r\n",
        "    objective='multi:softmax',\r\n",
        "    sampling_method='uniform',\r\n",
        "    num_class=num_class,\r\n",
        "    random_state=42,\r\n",
        ")\r\n",
        "\r\n",
        "pipeline = Pipeline(\r\n",
        "    steps=[\r\n",
        "        ('preprocessor', preprocessor),\r\n",
        "        ('classifier', classifier),\r\n",
        "    ]\r\n",
        ")\r\n",
        "\r\n",
        "params = {\r\n",
        "    'classifier__learning_rate': [0.1, 0.3],\r\n",
        "    'classifier__subsample': [0.7],\r\n",
        "    'classifier__colsample_bytree': [0.7],\r\n",
        "    'classifier__max_depth': [3, 4],\r\n",
        "    'classifier__n_estimators': [100],\r\n",
        "}\r\n",
        "\r\n",
        "classifiers['XGBClassifier'] = {\r\n",
        "    'pipeline': pipeline,\r\n",
        "    'params': params,\r\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1627579844400
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pipeline"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1627569760428
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid search cross validation\n",
        "\n",
        "- Use `balanced_accuracy` for scoring to deal with imbalanced classes\n",
        "- Loop over each classifier in the classifiers dictionary\n",
        "    - Train using training data\n",
        "    - Pickle trained model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for k, v in classifiers.items():\r\n",
        "\r\n",
        "    print(f\"\\nRunning grid search with cross validation for {k}...\")\r\n",
        "\r\n",
        "    gs = GridSearchCV(\r\n",
        "        v[\"pipeline\"],\r\n",
        "        v[\"params\"],\r\n",
        "        scoring=\"balanced_accuracy\",\r\n",
        "        cv=5,\r\n",
        "        n_jobs=-1,\r\n",
        "        verbose=2,\r\n",
        "    )\r\n",
        "\r\n",
        "    gs.fit(\r\n",
        "        X_train,\r\n",
        "        y_train,\r\n",
        "    )\r\n",
        "\r\n",
        "    joblib.dump(\r\n",
        "        gs,\r\n",
        "        Path(\r\n",
        "            \"sklearn\",\r\n",
        "            \"models\",\r\n",
        "            f\"{k}.pkl\",\r\n",
        "        ),\r\n",
        "    )"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation\n",
        "\n",
        "- Loop over each classifier in the classifiers dictionary\n",
        "    - Load model\n",
        "        - Store the grid search `best_score_`\n",
        "        - Store the grid search `best_params_`\n",
        "        - Store the grid search `best_estimator_`\n",
        "        - Store the grid search `best_estimator_` params\n",
        "    - Evalute using testing data\n",
        "        - Store the `balanced_accuracy_score` (average recall obtained on each class to deal with imbalance classes)\n",
        "        - Store class-wise `multilabel_confusion_matrix`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for k, v in classifiers.items():\r\n",
        "\r\n",
        "    gs = joblib.load(\r\n",
        "        Path(\r\n",
        "            \"sklearn\",\r\n",
        "            \"models\",\r\n",
        "            f\"{k}.pkl\",\r\n",
        "        ),\r\n",
        "    )\r\n",
        "\r\n",
        "    v[\"best_score\"] = gs.best_score_\r\n",
        "    v[\"best_params\"] = gs.best_params_\r\n",
        "    v[\"best_estimator\"] = gs.best_estimator_\r\n",
        "    v[\"best_estimator_params\"] = gs.best_estimator_.named_steps[\r\n",
        "        \"classifier\"\r\n",
        "    ].get_params()\r\n",
        "\r\n",
        "    print(f\"Running evaluation on test data for {k}...\")\r\n",
        "    y_pred = gs.predict(X_test)\r\n",
        "\r\n",
        "    v[\"testing_accuracy\"] = balanced_accuracy_score(\r\n",
        "        y_test,\r\n",
        "        y_pred,\r\n",
        "    )\r\n",
        "\r\n",
        "    v[\"testing_conf_matrix\"] = multilabel_confusion_matrix(\r\n",
        "        y_test,\r\n",
        "        y_pred,\r\n",
        "        samplewise=False,\r\n",
        "    )"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1627509914015
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voting classifier\n",
        "\n",
        "- Use `hard` voting ensamble of all the classifiers `best_estimator_`\n",
        "    - Classifiers should be passed as a list of `(str, estimator)`\n",
        "    - Train using training data-\n",
        "        - Store training `best_score_`        \n",
        "- Evalute using testing data\n",
        "    - Store the `balanced_accuracy_score` (average recall obtained on each class to deal with imbalance classes)\n",
        "    - Store class-wise `multilabel_confusion_matrix`\n",
        "- Pickle the classifiers dictionary\n",
        "- Balanced accuracy:\n",
        "    - Training: $97\\%$\n",
        "    - Testing: $90\\%$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "best_estimators = [(k, v[\"best_estimator\"]) for k, v in classifiers.items()]\r\n",
        "\r\n",
        "vc = VotingClassifier(\r\n",
        "    estimators=best_estimators,\r\n",
        "    voting=\"hard\",\r\n",
        ")\r\n",
        "\r\n",
        "print(\"Fitting VotingClassifier using all best estimators...\")\r\n",
        "vc.fit(X_train, y_train)\r\n",
        "\r\n",
        "y_pred = vc.predict(X_test)\r\n",
        "\r\n",
        "classifiers[\"VotingClassifier\"] = {\r\n",
        "    \"best_score\": vc.score(X_train, y_train),\r\n",
        "    \"testing_accuracy\": balanced_accuracy_score(\r\n",
        "        y_test,\r\n",
        "        y_pred,\r\n",
        "    ),\r\n",
        "    \"testing_conf_matrix\": multilabel_confusion_matrix(\r\n",
        "        y_test,\r\n",
        "        y_pred,\r\n",
        "        samplewise=False,\r\n",
        "    ),\r\n",
        "}\r\n",
        "\r\n",
        "joblib.dump(\r\n",
        "    vc,\r\n",
        "    Path(\r\n",
        "        \"sklearn\",\r\n",
        "        \"models\",\r\n",
        "        \"VotingClassifier.pkl\",\r\n",
        "    ),\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save results as flat file\n",
        "\n",
        "- Create DataFrame of results from all classifiers and save into cvs"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pd.DataFrame.from_dict(classifiers, orient=\"index\",).to_csv(\r\n",
        "    Path(\r\n",
        "        \"sklearn\",\r\n",
        "        \"models\",\r\n",
        "        \"models.csv\",\r\n",
        "    )\r\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1627543881333
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.2 64-bit ('.venv')"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "interpreter": {
      "hash": "506541454f2e71c9fdce11a02307324979c166298fc178c1da6e1bfef7c64bca"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}