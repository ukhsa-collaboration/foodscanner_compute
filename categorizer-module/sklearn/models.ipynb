{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kihJf_d8vAJQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (balanced_accuracy_score,\n",
    "                             multilabel_confusion_matrix)\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils.class_weight import (compute_class_weight,\n",
    "                                        compute_sample_weight)\n",
    "\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEmJKBD5vAJU"
   },
   "source": [
    "# Data preparation\n",
    "\n",
    "- Use `dtype` for performance\n",
    "- `ingredients` is a string of ingredients delimited with `|`, replace with `.`. Fill empty cells with `None` as the lack of ingredients is significant\n",
    "- `cooking_type` is a string of cooking types categories delimited with `|`, split into a list of categories. However, before splitting, fill empty cells with `None` as the lack of a cooking type is significant\n",
    "- Concatenate the not Null text columns: `description`, `regulated_product_name`, `ingredients` into new column `text`\n",
    "- Some products have duplicated `description`. To remove them, we set `pvid` as the index and sort it in an ascending order, then drop rows with duplicated `description` but keeping the one with the last `pvid` (i.e. the most recent product)\n",
    "- Drop rows with any empty cell. `ingredients` and `cooking_type` empty cells are now `None` so will not be dropped\n",
    "- Drop `description`, `regulated_product_name`, `ingredients` as these are now concatenated into `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EYnwlgfvAJV",
    "outputId": "5383bc0e-4426-4685-9ef1-c13214da3012",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "UInt64Index: 49623 entries, 6345061 to 8250896\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype   \n",
      "---  ------            --------------  -----   \n",
      " 0   category_level_1  49623 non-null  category\n",
      " 1   category_level_2  49623 non-null  category\n",
      " 2   storage_env       49623 non-null  category\n",
      " 3   pack_type         49623 non-null  category\n",
      " 4   cooking_type      49623 non-null  object  \n",
      " 5   label             49623 non-null  category\n",
      " 6   text              49623 non-null  object  \n",
      "dtypes: category(5), object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\n",
    "    os.path.join(\n",
    "        'data',\n",
    "        '200901_PHE_category_sheet.xlsx',\n",
    "    ),\n",
    "    usecols=[\n",
    "        'lProductVersionID',\n",
    "        'sDescription',\n",
    "        'sCategoryLevel1',\n",
    "        'sCategoryLevel2',\n",
    "        'regulated_product_name',\n",
    "        'ingredients',\n",
    "        'storage_env',\n",
    "        'pack_type',\n",
    "        'cooking_type',\n",
    "        'PHE_category_jan',\n",
    "    ],\n",
    "    dtype={\n",
    "        'lProductVersionID': 'uint64',\n",
    "        'sDescription': str,\n",
    "        'sCategoryLevel1': 'category',\n",
    "        'sCategoryLevel2': 'category',\n",
    "        'regulated_product_name': str,\n",
    "        'ingredients': str,\n",
    "        'storage_env': 'category',\n",
    "        'pack_type': 'category',\n",
    "        'cooking_type': str,\n",
    "        'PHE_category_jan': 'category',\n",
    "    },\n",
    ").rename(\n",
    "    columns={\n",
    "        'lProductVersionID': 'pvid',\n",
    "        'sDescription': 'description',\n",
    "        'sCategoryLevel1': 'category_level_1',\n",
    "        'sCategoryLevel2': 'category_level_2',\n",
    "        'PHE_category_jan': 'label',\n",
    "    }\n",
    ").assign(\n",
    "    ingredients=lambda df: df['ingredients'].str.replace(\n",
    "        '|', '.').fillna('None'),\n",
    "    cooking_type=lambda df: df['cooking_type'].fillna('None').str.rsplit('| '),\n",
    "    text=lambda\n",
    "    df: df[['description', 'regulated_product_name', 'ingredients']].apply(\n",
    "        lambda s: '. '.join(s[s.notna()]),\n",
    "        axis=1,\n",
    "    )\n",
    ").set_index(\n",
    "    'pvid',\n",
    ").sort_index(\n",
    "    ascending=True,\n",
    ").drop_duplicates(\n",
    "    subset='description',\n",
    "    keep='last',\n",
    ").dropna(\n",
    "    how='any',\n",
    ").drop(\n",
    "    ['description', 'regulated_product_name', 'ingredients'],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENWTcvF2vAJa"
   },
   "source": [
    "# Split data\n",
    "\n",
    "- Use `label` as target labels\n",
    "    - Number of classes is $63$\n",
    "    - **Classes are very imbalanced**\n",
    "        - Estimate class weights by using $n_{samples} / (n_{classes} * np.bincount(y))$ (not used but in case needed)\n",
    "        - Calculate sample weights (not used but in case needed)\n",
    "    - Stratify labels when splitting so their distribution in train/test data is similar\n",
    "- Encode target labels with values between $0-62$\n",
    "- Split $70/30$ for training/testing\n",
    "    - training shape: $(34736, 6)$\n",
    "    - testing shape: $(14887, 6)$\n",
    "- Create empty dict to store all classifiers. Once populated, an item will look like:\n",
    "`'classifier name' : {\n",
    "    'pipeline': ...,\n",
    "    'params': ...,\n",
    "    'best_score': ...,\n",
    "    'best_params': ...,\n",
    "    'best_estimator': ...,\n",
    "    'best_estimator_params': ...,\n",
    "    'testing_accuracy': ...,\n",
    "    'testing_conf_matrix': ...,\n",
    "}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8v5QWIdvAJb"
   },
   "outputs": [],
   "source": [
    "y = df['label']\n",
    "num_class = len(y.unique())\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=y.unique(),\n",
    "    y=y,\n",
    ")\n",
    "\n",
    "sample_weights = compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=y,\n",
    ")\n",
    "\n",
    "\n",
    "X = df.drop('label', axis=1)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "joblib.dump(\n",
    "    le,\n",
    "    os.path.join(\n",
    "        'models',\n",
    "        'LabelEncoder.pkl',\n",
    "    )\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "classifiers = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXhwizmnvAJf"
   },
   "source": [
    "## Transformers\n",
    "\n",
    "### Create 4 transformation `Pipeline`:\n",
    "\n",
    "1. A pipeline for text features: `text`. This will have 1 step:\n",
    "    - `TfidfVectorizer` which is equivalent to `CountVectorizer` followed by `TfidfTransformer`\n",
    "\n",
    "2. A pipeline for categorical features with 1 label each: `category_level_1`, `category_level_2`, `storage_env`. This will have 1 step:\n",
    "    - `OneHotEncoder`\n",
    "\n",
    "3. A pipeline for categorical features with multiple labels each: `cooking_type`. This will have 1 step:\n",
    "    - `CountVectorizer`\n",
    "\n",
    "4. A pipeline for categorical features to be hashed: `pack_type`. This will have 1 step:\n",
    "    - `FeatureHasher`. We used the hashing trick on `pack_type` as it has 45 categories and using `OneHotEncoder` would result in 45 sparse features\n",
    "    - Use a power of 2 for `n_features`\n",
    "    - Collisions are likely to cancel out rather than accumulate error when `alternate_sign=True`. However, MultinomialNB estimators expect non-negative inputs so will disable `alternate_sign`\n",
    "\n",
    "- `remainder=drop` will be used to drop any extra features that might be added to the dataframe later as a safety guard. When adding new features, either pass them through a pipeline, or change to `remainder=passthrough`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ppFQYu5vAJf"
   },
   "outputs": [],
   "source": [
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('tfidf', TfidfVectorizer(lowercase=True,\n",
    "                                  ngram_range=(1, 2),\n",
    "                                  norm='l2',\n",
    "                                  use_idf=True))\n",
    "    ]\n",
    ")\n",
    "\n",
    "cat_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('onehot', OneHotEncoder(categories='auto',\n",
    "                                 sparse=False,\n",
    "                                 handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "multi_cat_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('binarizer', CountVectorizer(analyzer=set))\n",
    "    ]\n",
    ")\n",
    "\n",
    "hash_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('hasher', FeatureHasher(n_features=10,\n",
    "                                 input_type='string',\n",
    "                                 alternate_sign=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_transformer, 'text'),\n",
    "        ('cat', cat_transformer, ['category_level_1',\n",
    "                                  'category_level_2', 'storage_env']),\n",
    "        ('multi_cat', multi_cat_transformer, 'cooking_type'),\n",
    "        ('hash', hash_transformer, 'pack_type'),\n",
    "    ],\n",
    "    remainder='drop',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BklcKN0qvAJi"
   },
   "source": [
    "### Check for Collisions\n",
    "\n",
    "- Check for possible collisions in the hashing features for `pack_type`\n",
    "- Increase `n_features` by 2 until no duplicated hashing features exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhK-01FavAJi",
    "outputId": "5b7b5655-e38c-4753-98dc-0a53b2e6c95e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashed_features = hash_transformer.named_steps['hasher'].fit_transform(\n",
    "    df['pack_type']\n",
    ").toarray()\n",
    "\n",
    "df_hashed_features = df[['pack_type']].reset_index(drop=True).join(\n",
    "    pd.DataFrame(hashed_features)\n",
    ")\n",
    "\n",
    "df_hashed_features.groupby(\n",
    "    ['pack_type']\n",
    ").first().duplicated(\n",
    "    keep=False\n",
    ").sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-X5Zc8ilvAJl"
   },
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEri89eJvAJm"
   },
   "source": [
    "## Multinomial Na√Øve Bayes\n",
    "\n",
    "- Normally requires bow\n",
    "- tf-idf vectors are also known to work well in practice\n",
    "- `alpha`: additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing)\n",
    "- Balanced accuracy:\n",
    "    - Training: $86\\%$\n",
    "    - Testing: $87\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlTn5CjovAJn"
   },
   "outputs": [],
   "source": [
    "classifier = MultinomialNB(alpha=1.0)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'classifier__alpha': [0.001, 0.01, 0.1, 0.3, 0.5, 1],\n",
    "}\n",
    "\n",
    "classifiers['MultinomialNB'] = {\n",
    "    'pipeline': pipeline,\n",
    "    'params': params,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_83qiVSvAJq"
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- `multinomial` logistic regression yields more accurate results and is faster to train on larger scale dataset than `ovr` logistic regression\n",
    "- `saga` solver works with `multinomial` and is faster for large datasets\n",
    "- `penalty=l1` trims the weights of not informative features to zero which is good if the goal is to extract the strongly discriminative vocabulary of each class. However, to get the best predictive accuracy, it is better to use `penalty=l2` instead\n",
    "- Balanced accuracy:\n",
    "    - Training: $90\\%$\n",
    "    - Testing: $90\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnCACelUvAJr"
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(\n",
    "    solver='saga',\n",
    "    class_weight='balanced',\n",
    "    multi_class='multinomial',\n",
    "    penalty='l2',\n",
    "    random_state=42,\n",
    "    max_iter=100,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'classifier__C': [1, 10, 100],\n",
    "}\n",
    "\n",
    "classifiers['LogisticRegression'] = {\n",
    "    'pipeline': pipeline,\n",
    "    'params': params,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5rb1rYZvAJv"
   },
   "source": [
    "## Linear Support Vector Machine\n",
    "\n",
    "- LinearSVC is Similar to SVC with parameter `kernel=linear` but scales better\n",
    "to large number of samples\n",
    "- Select `dual=True` when `n_samples` < `n_features`. The TfidfVectorizer creates a large sparse matrix as we are using uni-grams and bi-grams (more than 200K sparse features)\n",
    "- Select `multi_class=ovr` to train one-vs-rest classifiers\n",
    "- `class_weight=balanced`: automatically adjust weights inversely proportional to class frequencies\n",
    "- `C`: regularization parameter. Higher values result in less regularization (narrower margin with fewer violations)\n",
    "- Advantages\n",
    "    - Good for linear and non-linear classification\n",
    "    - Well suited for classification of complex but small to medium datasets\n",
    "- Disadvantages\n",
    "    - Speed and datasets size\n",
    "    - Needs scaling to be centred around zero\n",
    "    - Sensitive to outliers\n",
    "- Balanced accuracy:\n",
    "    - Training: $89\\%$\n",
    "    - Testing: $90\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxyOcd1HvAJw"
   },
   "outputs": [],
   "source": [
    "classifier = LinearSVC(\n",
    "    penalty='l2',\n",
    "    loss='squared_hinge',\n",
    "    # dual=True,\n",
    "    multi_class='ovr',\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'classifier__C': [1, 10, 100],\n",
    "    'classifier__dual': [True, False],\n",
    "}\n",
    "\n",
    "classifiers['LinearSVC'] = {\n",
    "    'pipeline': pipeline,\n",
    "    'params': params,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YUVYY-NvAJ0"
   },
   "source": [
    "## Random Forest Ensemble\n",
    "\n",
    "- Each tree is trained on a random subset of the training set with replacement (i.e. bootstrap aggregating or bagging)\n",
    "- Each tree is trained on a random subset of features, the number of features to use is defined by `max_features`\n",
    "- Advantages\n",
    "    - No need for scaling\n",
    "    - No need for dimensionality reduction (unless rotation is needed)\n",
    "- Disadvantages\n",
    "    - Sensitive to small variations in the training data\n",
    "    - Over-fitting\n",
    "    - Difficult to interpret\n",
    "- Balanced accuracy:\n",
    "    - Training: $79\\%$\n",
    "    - Testing: $78\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_6AHxU1vAJ0"
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'classifier__criterion': ['gini', 'entropy'],\n",
    "    'classifier__max_depth': [4, 5, 6],\n",
    "    'classifier__n_estimators': [100, 200, 400],\n",
    "}\n",
    "\n",
    "classifiers['RandomForestClassifier'] = {\n",
    "    'pipeline': pipeline,\n",
    "    'params': params,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfFn7bKLvAJ3"
   },
   "source": [
    "## Stochastic Extreme Gradient Boosting (XGBoost) Ensemble\n",
    "\n",
    "- Boosting methods train predictors sequentially, each trying to correct its predecessor. Gradient Boosting tries to fit the new predictor to the residual error made by the previous one. Extreme Boosting is more regularized to control over-fitting, which gives it better performance\n",
    "- XGBoost uses trees as the base booster by default (booster=`gbtree`), which has a sklearn API (other booster don't)\n",
    "- `learning_rate`: scales the contribution of each tree (ASA shrinkage). Lower `learning_rate` requires more trees to fit the training data\n",
    "- `subsample`: ratio of training data to be randomly sampled to train each tree (i.e. Stochastic). Typically, set `subsample >= 0.5` for good results when `sampling_method=uniform` which is the default\n",
    "- `colsample_bytree`: ratio of columns to be randomly sampled prior to train each tree\n",
    "- Balanced accuracy:\n",
    "    - Training: $83\\%$\n",
    "    - Testing: $85\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhZ_jpKNvAJ3"
   },
   "outputs": [],
   "source": [
    "classifier = xgb.XGBClassifier(\n",
    "    booster='gbtree',\n",
    "    objective='multi:softmax',\n",
    "    sampling_method='uniform',\n",
    "    num_class=num_class,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'classifier__learning_rate': [0.1, 0.3],\n",
    "    'classifier__subsample': [0.7, 0.9],\n",
    "    'classifier__colsample_bytree': [0.7, 0.9],\n",
    "    'classifier__max_depth': [3, 4],\n",
    "    'classifier__n_estimators': [100, 200, 400],\n",
    "}\n",
    "\n",
    "classifiers['XGBClassifier'] = {\n",
    "    'pipeline': pipeline,\n",
    "    'params': params,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yABc0mTavAJ7"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcM47m5JvAJ8"
   },
   "source": [
    "## Grid search cross validation\n",
    "\n",
    "- Use `balanced_accuracy` for scoring to deal with imbalanced classes\n",
    "- Loop over each classifier in the classifiers dictionary\n",
    "    - Train using training data\n",
    "    - Pickle trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZA2bNUw_vAJ9"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('models'):\n",
    "    os.mkdir('models')\n",
    "\n",
    "for k, v in classifiers.items():\n",
    "\n",
    "    print(f'\\nRunning grid search with cross validation for {k}...')\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        v['pipeline'],\n",
    "        v['params'],\n",
    "        scoring='balanced_accuracy',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    gs.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "    )\n",
    "\n",
    "    joblib.dump(\n",
    "        gs,\n",
    "        os.path.join(\n",
    "            'models',\n",
    "            f'{k}.pkl',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guIPzpInvAKA"
   },
   "source": [
    "# Validation\n",
    "\n",
    "- Loop over each classifier in the classifiers dictionary\n",
    "    - Load model\n",
    "        - Store the grid search `best_score_`\n",
    "        - Store the grid search `best_params_`\n",
    "        - Store the grid search `best_estimator_`\n",
    "        - Store the grid search `best_estimator_` params\n",
    "    - Evalute using testing data\n",
    "        - Store the `balanced_accuracy_score` (average recall obtained on each class to deal with imbalance classes)\n",
    "        - Store class-wise `multilabel_confusion_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBbsbFe2vAKB"
   },
   "outputs": [],
   "source": [
    "for k, v in classifiers.items():\n",
    "\n",
    "    gs = joblib.load(\n",
    "        os.path.join(\n",
    "            'models',\n",
    "            f'{k}.pkl',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    v['best_score'] = gs.best_score_\n",
    "    v['best_params'] = gs.best_params_\n",
    "    v['best_estimator'] = gs.best_estimator_\n",
    "    v['best_estimator_params'] = gs.best_estimator_.named_steps['classifier'].get_params()\n",
    "\n",
    "    print(f'Running evaluation on test data for {k}...')\n",
    "    y_pred = gs.predict(X_test)\n",
    "\n",
    "    v['testing_accuracy'] = balanced_accuracy_score(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "    )\n",
    "\n",
    "    v['testing_conf_matrix'] = multilabel_confusion_matrix(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        samplewise=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0BKaJ1vvAKD"
   },
   "source": [
    "## Voting classifier\n",
    "\n",
    "- Use `hard` voting ensamble of all the classifiers `best_estimator_`\n",
    "    - Classifiers should be passed as a list of `(str, estimator)`\n",
    "    - Train using training data-\n",
    "        - Store training `best_score_`        \n",
    "- Evalute using testing data\n",
    "    - Store the `balanced_accuracy_score` (average recall obtained on each class to deal with imbalance classes)\n",
    "    - Store class-wise `multilabel_confusion_matrix`\n",
    "- Pickle the classifiers dictionary\n",
    "- Balanced accuracy:\n",
    "    - Training: $97\\%$\n",
    "    - Testing: $90\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYrgWQlfvAKE"
   },
   "outputs": [],
   "source": [
    "best_estimators = [(k, v['best_estimator'])\n",
    "                   for k, v in classifiers.items()]\n",
    "\n",
    "vc = VotingClassifier(\n",
    "    estimators=best_estimators,\n",
    "    voting='hard',\n",
    ")\n",
    "\n",
    "print('Fitting VotingClassifier using all best estimators...')\n",
    "vc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "classifiers['VotingClassifier'] = {\n",
    "\n",
    "    'best_score': vc.score(X_train, y_train),\n",
    "\n",
    "    'testing_accuracy': balanced_accuracy_score(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "    ),\n",
    "\n",
    "    'testing_conf_matrix': multilabel_confusion_matrix(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        samplewise=False,\n",
    "    )\n",
    "}\n",
    "\n",
    "joblib.dump(\n",
    "    vc,\n",
    "    os.path.join(\n",
    "        'models',\n",
    "        'VotingClassifier.pkl',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeFBPsKBvAKG"
   },
   "source": [
    "### Save results as flat file\n",
    "\n",
    "- Create DataFrame of results from all classifiers and save into cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yB20na2KvAKH"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(\n",
    "    classifiers,\n",
    "    orient='index',\n",
    ").to_csv(\n",
    "    os.path.join(\n",
    "        'models',\n",
    "        'models.csv',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVKWvE-7vAKK"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9D1oKObpvAKL"
   },
   "source": [
    "### Read example JSON file\n",
    "\n",
    "- Parse out all the features as expected by trained models\n",
    "    - `category_level_1`: string category\n",
    "    - `category_level_2`: string category\n",
    "    - `regulated_product_name`: string\n",
    "    - `ingredients`: list of strings. Join with '. '\n",
    "    - `text`: concatenated from `description`, `regulated_product_name`, and `ingredients` with '. '\n",
    "    - `storage_env`: string category\n",
    "    - `pack_type`: string category\n",
    "    - `cooking_type`: a list of categories that only exists if there are cooking types. If it does exist, return the list, otherwise, return a list of 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n71TBZvUvAKM"
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\n",
    "    os.path.join(\n",
    "        'data',\n",
    "        'trial-json-products.json',\n",
    "    ),\n",
    "    orient='records',\n",
    "    encoding='utf-16',\n",
    "    lines=False,\n",
    ").set_index(\n",
    "    'pvid',\n",
    ").sort_index(\n",
    "    ascending=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhDmzRVIvAKO"
   },
   "outputs": [],
   "source": [
    "df['category_level_1'] = df['categories'].apply(\n",
    "    lambda\n",
    "    c: c[0]['description'],\n",
    ")\n",
    "\n",
    "df['category_level_2'] = df['categories'].apply(\n",
    "    lambda\n",
    "    c: c[1]['description'],\n",
    ")\n",
    "\n",
    "df['regulated_product_name'] = df['languages'].apply(\n",
    "    lambda\n",
    "    c: c[0]['groupingSets'][0]['attributes']['regulatedProductName']\n",
    ")\n",
    "\n",
    "df['ingredients'] = df['languages'].apply(\n",
    "    lambda\n",
    "    c: '. '.join(\n",
    "        c[0]['groupingSets'][0]['attributes']['ingredients']\n",
    "    )\n",
    ")\n",
    "\n",
    "df['text'] = df[\n",
    "    ['description', 'regulated_product_name', 'ingredients']\n",
    "].apply(\n",
    "    lambda s: '. '.join(s[s.notna()]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df['storage_env'] = df['languages'].apply(\n",
    "    lambda\n",
    "    c: c[0]['groupingSets'][0]['attributes']['storageType'][0]\n",
    "    ['lookupValue']\n",
    ")\n",
    "\n",
    "df['pack_type'] = df['languages'].apply(\n",
    "    lambda\n",
    "    c: c[0]['groupingSets'][0]['attributes']['packType'][0]\n",
    "    ['lookupValue']\n",
    ")\n",
    "\n",
    "\n",
    "def parse_cooking_guidelines(c):\n",
    "    try:\n",
    "        return [\n",
    "            item['nameValue']\n",
    "            for item in c[0]['groupingSets'][0]['attributes']\n",
    "            ['cookingGuidelines']\n",
    "        ]\n",
    "\n",
    "    except KeyError:\n",
    "        return ['None']\n",
    "\n",
    "\n",
    "df['cooking_type'] = df['languages'].apply(\n",
    "    parse_cooking_guidelines\n",
    ")\n",
    "\n",
    "df = df[[\n",
    "    'category_level_1',\n",
    "    'category_level_2',\n",
    "    'storage_env',\n",
    "    'pack_type',\n",
    "    'cooking_type',\n",
    "    'text'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bas340izvAKR"
   },
   "source": [
    "#### - Load label encoder to get label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDBU_pIvvAKR"
   },
   "outputs": [],
   "source": [
    "le = joblib.load(\n",
    "    os.path.join(\n",
    "        'models',\n",
    "        'LabelEncoder.pkl',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSvhSAjPvAKT"
   },
   "source": [
    "#### - Load VotingClassifier\n",
    "- Make predictions\n",
    "- Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1gn7r2uvAKT"
   },
   "outputs": [],
   "source": [
    "vc = joblib.load(\n",
    "    os.path.join(\n",
    "        'models',\n",
    "        'VotingClassifier.pkl',\n",
    "    )\n",
    ")\n",
    "\n",
    "df['predict'] = le.inverse_transform(vc.predict(df))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "models.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
